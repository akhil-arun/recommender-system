{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-27T22:33:21.073622Z",
     "start_time": "2025-04-27T22:33:21.061451Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from data_utils.data_utils import set_seed\n",
    "set_seed(seed=42)\n",
    "from data_utils.preprocess import clean_and_filter\n",
    "\n",
    "# compute the absolute path to your project root:\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# insert it at the front of Python’s module search path:\n",
    "sys.path.insert(0, root)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T22:33:22.413473Z",
     "start_time": "2025-04-27T22:33:21.189188Z"
    }
   },
   "id": "654cfcf99cfa7fc6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset wrapper\n",
    "from data_utils.datasets import SASRTrainDataset\n",
    "\n",
    "# Model and evaluation\n",
    "from models.sasr import SASR\n",
    "from evaluation import evaluate_ranking_model, evaluate_featureaware_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T22:33:22.632357Z",
     "start_time": "2025-04-27T22:33:22.414549Z"
    }
   },
   "id": "a9157eaa05df52f7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Running on\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T22:33:23.485235Z",
     "start_time": "2025-04-27T22:33:23.438602Z"
    }
   },
   "id": "d0094ab43d7d47"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserID': 0, 'prefix': [3118, 1251], 'positive': 1673, 'negatives': [2621, 457, 103, 3039, 1127], 'padded_prefix': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3118, 1251], 'mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from data_utils.preprocess import load_movielens, get_user_sequences, split_sequences\n",
    "import json\n",
    "\n",
    "def load_json_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads JSON data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the JSON data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{file_path}'\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in '{file_path}'\")\n",
    "        return None\n",
    "\n",
    "file_path = '../data/'\n",
    "\n",
    "\n",
    "ratings, users, movies = load_movielens(file_path)\n",
    "ratings, users, movies = clean_and_filter(ratings, users, movies, rating_threshold=4)\n",
    "user_seqs = get_user_sequences(ratings)\n",
    "\n",
    "user_splits = split_sequences(user_seqs, train_ratio=0.8, val_ratio=0.1)\n",
    "\n",
    "train_exs = load_json_from_file(f'{file_path}train_data.json')\n",
    "val_exs = load_json_from_file(f'{file_path}val_data.json')\n",
    "test_exs = load_json_from_file(f'{file_path}test_data.json')\n",
    "\n",
    "all_movies = set(movies[\"MovieID\"].unique())\n",
    "all_users = set(users[\"UserID\"].unique())\n",
    "num_total_movies = len(all_movies)\n",
    "num_total_users = len(all_users)\n",
    "\n",
    "print(train_exs[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T22:33:37.017723Z",
     "start_time": "2025-04-27T22:33:25.162656Z"
    }
   },
   "id": "5b5dc813dd369c46"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_ds = SASRTrainDataset(train_exs, num_negatives=1)\n",
    "val_ds   = SASRTrainDataset(val_exs,   num_negatives=1)\n",
    "\n",
    "sasr = SASR(num_total_users, num_total_movies, d_model=64, n_head=2, num_layers=2).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "optimizer = optim.Adam(sasr.parameters(), lr=1e-4)\n",
    "\n",
    "def bpr_loss(sasr, encoded_seq_next, pos, neg):\n",
    "    \n",
    "    pos_scores = torch.sum(encoded_seq_next * sasr.item_embeddings(pos), -1)\n",
    "    neg_scores = torch.sum(encoded_seq_next * sasr.item_embeddings(neg), -1)\n",
    "    \n",
    "    return -(pos_scores - neg_scores).sigmoid().log().mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T20:55:47.250040Z",
     "start_time": "2025-04-27T20:55:47.120930Z"
    }
   },
   "id": "f8fcbb4e0a13601e"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — Train: 3.3994, Val: 3.1176\n",
      "Epoch 2/5 — Train: 2.6655, Val: 2.5147\n",
      "Epoch 3/5 — Train: 2.1278, Val: 2.0345\n",
      "Epoch 4/5 — Train: 1.6958, Val: 1.6551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x124c8c400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/akshayd/miniconda3/envs/rec/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # ——— train ———\n",
    "    sasr.train()\n",
    "    tot_train = 0.0\n",
    "    for user, pos, neg, prefix in train_loader:\n",
    "        user, pos, neg, prefix = (user.to(device), pos.to(device), neg.to(device), prefix.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        encoded_seq_next = sasr(user, prefix)[:, -1, :]\n",
    "        loss = bpr_loss(sasr, encoded_seq_next, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_train += loss.item()\n",
    "    avg_train = tot_train / len(train_loader)\n",
    "    train_losses.append(avg_train)\n",
    "\n",
    "    # ——— val ———\n",
    "    sasr.eval()\n",
    "    tot_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for user, pos, neg, prefix in val_loader:\n",
    "            user, pos, neg, prefix = user.to(device), pos.to(device), neg.to(device), prefix.to(device)\n",
    "            encoded_seq_next = sasr(user, prefix)[:, -1, :]\n",
    "            tot_val += bpr_loss(sasr, encoded_seq_next, pos, neg).item()\n",
    "    avg_val = tot_val / len(val_loader)\n",
    "    val_losses.append(avg_val)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{epochs} — Train: {avg_train:.4f}, Val: {avg_val:.4f}\")\n",
    "\n",
    "# Cell 6: plot curves\n",
    "plt.plot(range(1, epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, epochs+1), val_losses,   label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BPR Loss\")\n",
    "plt.title(\"Training & Validation Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T21:14:32.853921Z",
     "start_time": "2025-04-27T20:55:49.015943Z"
    }
   },
   "id": "7b52176991e37cb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83271468c3e95dc3"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "- Hit@10: 0.5359\n",
      "- Hit@10 Std: 0.0023\n",
      "- NDCG@10: 0.2934\n",
      "- NDCG@10 Std: 0.0020\n",
      "- MRR: 0.2424\n",
      "- MRR Std: 0.0021\n",
      "- MAP: 0.2424\n",
      "- MAP Std: 0.0021\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_ranking_model(\n",
    "    model=sasr,\n",
    "    user_splits=user_splits,\n",
    "    global_items=all_movies,\n",
    "    device=device,\n",
    "    candidate_size=100,\n",
    "    k=10,\n",
    "    model_type='sasr'\n",
    ")\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "for name, val in metrics.items():\n",
    "    print(f\"- {name}: {val:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T20:46:58.374021Z",
     "start_time": "2025-04-27T20:42:39.585327Z"
    }
   },
   "id": "99981ba034e59cf4"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MovieID                                      Title  similarity\n",
      "0           0                           Toy Story (1995)    1.000000\n",
      "257       257  Star Wars: Episode IV - A New Hope (1977)    0.275273\n",
      "589       589           Silence of the Lambs, The (1991)    0.273623\n",
      "847       847                      Godfather, The (1972)    0.251519\n",
      "1015     1015                        Mary Poppins (1964)    0.248279\n",
      "1180     1180             Raiders of the Lost Ark (1981)    0.247023\n",
      "1195     1195                          GoodFellas (1990)    0.245134\n",
      "1220     1220                     Terminator, The (1984)    0.244651\n",
      "1726     1726                  As Good As It Gets (1997)    0.244471\n",
      "2789     2789                     American Beauty (1999)    0.240619\n",
      "2890     2890                          Fight Club (1999)    0.230823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bw/0d70cdj52fn0jt8hp7qxqj5r0000gn/T/ipykernel_23552/2507088295.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  similar_movies_df['similarity'] = scores\n"
     ]
    }
   ],
   "source": [
    "# toy_story_emb = sasr.item_embeddings.weight[1]               # shape: [32]\n",
    "# item_embs = F.normalize(sasr.item_embeddings.weight, dim=1)  # shape: [3883, 32]\n",
    "# toy_story_emb = toy_story_emb / toy_story_emb.norm()       # shape: [32]\n",
    "# similarities = item_embs @ toy_story_emb  # shape: [3883]\n",
    "# # Step 4: Get top-k most similar item indices (excluding itself if needed)\n",
    "# topk = torch.topk(similarities, k=10 + 1)\n",
    "# \n",
    "# # ## Adding 1 here because 0 is my padding index\n",
    "# # movie_id_to_index = {movie_id: i + 1 for i, movie_id in enumerate(movies['MovieID'].unique())}\n",
    "# # index_to_movie_id = {v: k for k, v in movie_id_to_index.items()}\n",
    "# \n",
    "# indices = topk.indices.tolist()\n",
    "# # # Look them up in the movies DataFrame\n",
    "# mapped_movies = []\n",
    "# for values in indices:\n",
    "#     mapped_movies.append(index_to_movie_id[values])\n",
    "# similar_movies_df = movies[movies['MovieID'].isin(mapped_movies)]\n",
    "# # # Optional: Sort by similarity score\n",
    "# scores = similarities[indices].tolist()\n",
    "# similar_movies_df['similarity'] = scores\n",
    "# similar_movies_df = similar_movies_df.sort_values(by='similarity', ascending=False)\n",
    "# \n",
    "# print(similar_movies_df[['MovieID', 'Title', 'similarity']])\n",
    "\n",
    "from torch.nn import functional as F\n",
    "toy_story_emb = sasr.item_embeddings.weight[0]               # shape: [32]   \n",
    "item_embs = F.normalize(sasr.item_embeddings.weight, dim=1)  # shape: [3883, 32]\n",
    "toy_story_emb = toy_story_emb / toy_story_emb.norm()       # shape: [32]\n",
    "similarities = item_embs @ toy_story_emb  # shape: [3883]\n",
    "# Step 4: Get top-k most similar item indices (excluding itself if needed)\n",
    "topk = torch.topk(similarities, k=10 + 1)\n",
    "indices = topk.indices.tolist()\n",
    "# Look them up in the movies DataFrame\n",
    "similar_movies_df = movies[movies['MovieID'].isin(indices)]\n",
    "# Optional: Sort by similarity score\n",
    "scores = similarities[indices].tolist()\n",
    "similar_movies_df['similarity'] = scores\n",
    "similar_movies_df = similar_movies_df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "print(similar_movies_df[['MovieID', 'Title', 'similarity']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-27T20:47:18.245719Z",
     "start_time": "2025-04-27T20:47:18.117632Z"
    }
   },
   "id": "b6e5c4fcb6ef4b11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2f820a30854c4ad1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
